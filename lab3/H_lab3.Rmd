---
title: "Computer Lab 3"
author: "Ravinder Reddy Atla"
date: "5/5/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```


## 1. Gibbs Sampler for a normal model

```{r}
rainfall_data <- read.table('rainfall.dat')
```

### (a) Simulation of Gibbs sampler from joint posterior distribution

```{r}
# Data and prior Initialization
log_y <- as.matrix(log(rainfall_data))
mu0 <- 0
tau0_2 <- 1
v0 <- 1
si0_2 <- 1

# Initial value
si_2 <- 1
n_iterations <- 1000
mu_vec <- rep(NA,n_iterations)
si_vec <- rep(NA,n_iterations)
logy_post <- rep(NA,n_iterations)
n <- length(log_y)

# Pre calculation
log_y_mean <- mean(log_y)

# Function for getting samples from inverse chisquare distributino
rinvchisquare <- function(num_draws, n, tau_sq){
  #set.seed(1234)
  x <- rchisq(num_draws,df = n)
  x_inv <- ((n)*tau_sq)/x
  return(x_inv)
}

# Gibbs sampling iterations
for(k in 1:n_iterations){
  # Sampling mu
  w <- (n/si_2)/((n/si_2) + (1/tau0_2))
  taun_2 <- w/(n/si_2)
  mun <- (w * log_y_mean) + ((1 - w) * mu0)
  mu <- rnorm(1, mun, sqrt(taun_2))

  # Sampling si_2
  vn <- v0+n
  sin_2 <- (v0*si0_2 + sum((log_y - mu)^2))/vn
  si_2 <- rinvchisquare(1, vn, sin_2)
  
  # sample from posterior predictive
  logy_post[k] <- rnorm(1, mu, sqrt(si_2))
  
  mu_vec[k] <- mu
  si_vec[k] <- si_2
}

```

```{r}
library(MASS)

rho_mu <- acf(mu_vec)
rho_si <- acf(si_vec)

IF_mu <- 1 + 2*sum(rho_mu$acf)
IF_si <- 1 + 2*sum(rho_si$acf)

cat('Inefficiency factor for parameter mu : ',IF_mu,'\n')
cat('Inefficiency factor for parameter mu : ',IF_si)

par(mfrow = c(1,2))
plot(x = 1:n_iterations, y = mu_vec,type = 'l', col = 'lightblue3',
     xlab = 'Iterations', ylab = 'mu')
plot(x = 1:n_iterations, y = si_vec,type = 'l', col = 'lightblue3',
     xlab = 'Iterations', ylab = 'si')
```

### (b). Posterior Predictive density

```{r}
y_post <- exp(logy_post)
y_prior <- as.matrix(rainfall_data)

plot(density(y_prior),lwd = 4, main = 'Daily precipitation vs Posterior Predictive')
lines(density(y_post),col = 'orange',lwd = 2)
legend(300, 0.020,
       legend = c('Prior','Posterior Predictive'),
       col = c('black','orange'),
       lty=c(1,1),lwd = 3)

```


## 2. Metropolis Random Walk for Poisson regression

```{r}
ebay_data <- read.table('ebayNumberOfBidderData.dat',header = TRUE)
rows <- nrow(ebay_data)
cols <- ncol(ebay_data)
y <- as.matrix(ebay_data[1])
X <- as.matrix(ebay_data[,2:cols])
```


### (a). Maximum Likelihood Estimator of beta

```{r}
x_glm <- X[,2:ncol(X)]
model <- glm(y ~ x_glm,family = poisson())
print(model$coefficients)
```

As observed from the coefficients of the covariates, MinBidShare coefficient is much lower when compared to the others hence, negatively affecting the prediction. Sealed variable covariate is the only covariate with positive coefficient which might have significant positive effect on regression.


### (b). Bayesian Analysis on Poisson Regression

```{r}
library(mvtnorm)
params <- dim(X)[2]
mu <- as.matrix(rep(0,params))
Sigma = 100*solve(t(X)%*%X)

LogPostPoisson <- function(betas,y,X,mu,Sigma){
  n <- nrow(X)
  lamda <- exp(X%*%betas)
  logLik <- (sum((X%*%betas)*y) - sum(lamda))
  Prior <- dmvnorm(betas, mu, Sigma, log=TRUE)
  return(logLik + Prior)
}

initValue <- matrix(0,params,1)

# Optimum beta(coefficient) are calculated
OptimRes <- optim(initValue,
                  LogPostPoisson, gr=NULL, y,X, mu, Sigma, method=c("BFGS"),
                  control=list(fnscale=-1), hessian=TRUE)
```

```{r}
print('Posterior Mode: ')
print(OptimRes$par)
print('Inverse of hessian matrix')
inversehessian <- solve(-OptimRes$hessian)
print(inversehessian)
```


### (c). Metropolis Algorithm


```{r}
logPosteriorPoisson <- function(betas){
  params <- ncol(X)
  mu <- as.matrix(rep(0,params))
  Sigma = 100*solve(t(X)%*%X)
  
  n <- nrow(X)
  lamda <- exp(X%*%betas)
  logLik <- (sum((X%*%betas)*y) - sum(lamda))
  Prior <- dmvnorm(t(betas), mu, Sigma, log=TRUE)
  return(logLik + Prior)
}

sampleFromPosterior <- function(num_iterations, theta, c, postDensityFun){
  theta_current <- theta
  samples <- matrix(theta_current, num_iterations, nrow(theta))
  accept <- c()
  accept[1] <- 1
  for(i in 2:num_iterations){
    shift <- t(rmvnorm(1, mean = theta_current, sigma = c*inversehessian))
    theta_new <- samples[i-1,] + shift
    
    p_log_target_val <- postDensityFun(theta_new)
    log_target_val <- postDensityFun(theta_current)
    
    alpha = min(1, exp(p_log_target_val - log_target_val))
    if(log(runif(1)) <= alpha){
      #theta_current <- theta_new
      samples[i,] <- theta_new
      accept[i]<-1
    }
    else{
      samples[i,] <- samples[i-1,]
      accept[i]<-0
    }
  }
  print(sum(accept)/n_iterations)
  return(samples)
}
set.seed(1234)
beta_init <- matrix(0,dim(X)[2],1)
iter = 100
sample_bet <- sampleFromPosterior(iter, beta_init, 0.1, logPosteriorPoisson)

for(i in 1:ncol(sample_bet)){
  plot(c(1:iter),sample_bet[,i],'l',
       ylab = paste('beta',i), xlab = 'Num Samples')
}
```



### (d)

```{r}
input <- c(1,1,1,1,0,1,0,1,0.7)
pred <- c()
posteriorPredictive <- function(lamda,y1){
  return((exp(-lamda))*lamda^y1)
}
for(i in 1:nrow(sample_bet)){
  lambda <- exp(input%*%t(sample_bet[i]))
  y1 <- 0
  pred[i] <- posteriorPredictive(lambda, y1)
}
print(pred)
```

## 3. Time Series Models in Stan

### (a). Simulate from AR process (R):

```{r}
mu <- 20
si2 <- 4
t <- 200


```






