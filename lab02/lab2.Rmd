---
title: "Lab 2 Ravinder"
author: "Ravinder Reddy Atla"
date: "4/21/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = TRUE)
knitr::opts_chunk$set(message = TRUE)
```

# 1. Linear and polynomial regression

```{r}
library(mvtnorm)
```

```{r}


```


```{r}
# Scaled Inverse chi square distribution
pdfinvchisquare <- function(x,n,tau_sq){
  res <- (((tau_sq*(n-1))/2)^(n-1)/2 * exp(-(tau_sq*(n-1))/(2*x)))/((x^(1+(n-1)/2)) * gamma((n-1)/2))
  return(res)
}

# Function for sampling from inverse chi square distribution
rinvchisquare <- function(num_draws, n, tau_sq){
  set.seed(1234)
  x <- rchisq(num_draws,df = n-1)
  x_inv <- ((n-1)*tau_sq)/x
  return(x_inv)
}
```

```{r}
# Prior parameters initialization
sisquare0 <- 1
v0 <- 4
mu0 <- c(-10, 100, -100)
si0 <- 0.01*diag(3)
si0_inv <- solve(si0)
num_draws = 1000
# Prior hierarchy

# Prior 1: si^2
var_prior <- rinvchisquare(num_draws, v0, sisquare0)

# Prior 2: beta/si^2
beta_prior <- matrix(NA, nrow = num_draws, ncol = 3)
temp <- c()
for(i in 1:num_draws){
  beta_prior[i,] <- rmvnorm(1,mean = mu0, sigma = var_prior[i]*si0_inv)
  #temp[i] <- 
}


```


# 2. Posterior approximation for classification with logistic regression

```{r}
ww_data <- read.table('WomenWork.dat', header = TRUE)
rows <- nrow(ww_data)
cols <- ncol(ww_data)
```

```{r}
y <- as.matrix(ww_data[1])
X <- as.matrix(ww_data[,2:cols])
```

```{r}
params <- dim(X)[2]
mu <- as.matrix(rep(0,params))
tau = 10
Sigma = (tau^2)*diag(params)

LogPostLogistic <- function(betas,y,X,mu,Sigma){
  linPred <- X%*%betas;
  logLik <- sum(linPred*y - log(1 + exp(linPred)))
  logPrior <- dmvnorm(betas, mu, Sigma, log=TRUE)
  
  return(logLik + logPrior)
}

initValue <- matrix(0,params,1)

OptimRes <- optim(initValue,
                  LogPostLogistic, gr=NULL, y, X, mu, Sigma, method=c("BFGS"),
                  control=list(fnscale=-1), hessian=TRUE)
```


```{r}
print('Posterior Mode: ')
print(OptimRes$par)
print('Inverse of hessian matrix')
inversehessian <- solve(OptimRes$hessian)
print(inversehessian)
```


```{r}
beta_post <- OptimRes$par
names(beta_post) <- colnames(ww_data[,2:cols])
approx_par_NSC <- rnorm(1000,beta_post['NSmallChild'],-inversehessian[7,7])
lowerInterval <- quantile(approx_par_NSC,0.05)
upperInterval <- quantile(approx_par_NSC, 0.95)
```

```{r}
plot(density(approx_par_NSC),lwd = 3,main = '95% Posterior Probability Interval of NSmallChild variable')
polygon(density(approx_par_NSC), col = 'lightblue2')
abline(v = lowerInterval, col = 'red', lwd = 3)
abline(v = upperInterval, col = 'red', lwd = 3)
arrows(lowerInterval,0.3,upperInterval,0.3,length = 0.1,col = 'black',lwd = 3)
arrows(upperInterval,0.3,lowerInterval,0.3,length = 0.1,col = 'black',lwd = 3)
text(-1.4,0.5, '95% CI', lwd = 3,cex = 1.3)

```



### b.

```{r}
posteriorPredictive <- function(x, beta_post, inversehessian){
  post_sample <- rmvnorm(1, beta_post,-inversehessian)
  #print(x)
  #print(x %*% t(post_sample))
  logist_prob <- (exp(x %*% t(post_sample)))/(1 + exp(x %*% t(post_sample)))
  #print(logist_prob)
  return(logist_prob)
}

```

```{r}
x <- c(1,13, 8, 11, (11/10)^2, 37, 2, 0)
nsamples = 1000
post_predict <- c(rep(0,nsamples))
for(i in 1:nsamples){
  post_predict[i] <- posteriorPredictive(x, beta_post, inversehessian)
}
#print(post_predict)
hist(post_predict, breaks = 100, 
     col = 'lightblue2',lwd = 3,
     xlab = 'Pr(y=1|X)',
     main = 'Posterior Predictive Plot')
lines(density(post_predict), col = 'red',lwd = 3)
polygon(density(post_predict), col = rgb(red = 0, green = 0, blue = 1, alpha = 0.5))
```
 


# c. 

```{r}
posteriorPredictiveBinomial <- function(x, beta_post, inversehessian){
  post_sample <- rmvnorm(1, beta_post,-inversehessian)
  logist_prob1 <- (exp(x %*% t(post_sample)))/(1 + exp(x %*% t(post_sample)))
  logist_prob <- sum(rbinom(1,8,logist_prob1))
  return(logist_prob)
}

```

```{r}
test_data <- matrix(x,nrow=8, ncol=8,byrow = TRUE)
nsamples = 1000
post_predict <- c(rep(0,nsamples))
for(i in 1:nsamples){
  post_predict[i] <- posteriorPredictiveBinomial(test_data, beta_post, inversehessian)
}
#barplot(post_predict,col = 'cadetblue')
#plot(density(post_predict))
hist(post_predict,breaks = 100,col = 'lightblue2',
     xlab = 'Number of women who works',
     main = 'Histogram of number of women who works ')
```




















